1) build a AI-NN Problem for classification problem
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# Display the first image in the training set
plt.imshow(x_train[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.show()



# Define the model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')  # 10 classes for MNIST digits
])

# Compile the model
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# Train the model
model.fit(x_train, y_train, epochs=2)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc}")


-----------------------------------------------------------------------------------------------------------------------

2) Construct DL Model for find the efficient hyperparameters
!pip install keras-tuner
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import keras_tuner as kt
#define model building function
def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28)))
# tuning the number of hidden layers and neurons
    for i in range(hp.Int('num_layers', 1,3)):
      model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                          min_value=32,
                                          max_value=512,
                                          step=32),
                                          activation=hp.Choice('activation_' + str(i),
                                                              ['relu', 'tanh', 'sigmoid'])))
      model.add(layers.Dense(10, activation='softmax'))
      model.compile(
          optimizer=keras.optimizers.Adam(
              learning_rate=hp.Choice('learning_rate', [0.01,0.001,0.0001])),
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])
      return model
#load data(MNIST dataset as an example)
(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()
x_train,x_test=x_train/255.0,x_test/255.0
#Normalize
# Create a tuner
tuner=kt.Hyperband(
    build_model,
    objective='val_accuracy',
    max_epochs=10,
    factor=3,
    directory='my_dir',
    project_name='intro_to_kt'
)
tuner.search(x_train,y_train,epochs=10,validation_split=0.2)
#get best hyperparametrs amd model
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]
model=tuner.hypermodel.build(best_hps)
model.fit(x_train,y_train,epochs=10,validation_split=0.2)
#Evaluate model
eval_result=model.evaluate(x_test,y_test)
print(f"Test Loss: {eval_result[0]}, Test Accuracy: {eval_result[1]*100:.2f}%")
#print best hyperparameters
print("best hyperparameters are:")
for hp_name, hp_value in best_hps.values.items():
    print(f"{hp_name}: {hp_value}")



-----------------------------------------------------------------------------------------------------------------------


3)

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'I love programming in Python',
    'Python programming is fun',
    'I love coding in Python',
    'I enjoy learning new programming languages',
    'Programming in Python is great'
]

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(corpus)

tfidf_array = X.toarray()

feature_names = vectorizer.get_feature_names_out()

print("TF-IDF Representation:")
print(tfidf_array)

print("\nFeature Names (Words):")
print(feature_names)

for i, doc in enumerate(tfidf_array):
    print(f"\nDocument {i + 1}:")
    for j, score in enumerate(doc):
        if score > 0:
            print(f"{feature_names[j]}: {score:.4f}")



-----------------------------------------------------------------------------------------------------------------------


4)


import tensorflow as tf
from tensorflow.keras.layers import Input,Dense
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

(x_train,_),(x_test,_) = mnist.load_data()
x_train = x_train.astype('float32')/255.
x_test = x_test.astype('float32')/255.
x_train = x_train.reshape((x_train.shape[0],-1))
x_test = x_test.reshape((x_test.shape[0],-1))

input_dim = x_train.shape[1]
encoding_dim = 64

input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim,activation='relu')(input_layer)
decoded = Dense(input_dim,activation='sigmoid')(encoded)

autoencoder = Model(input_layer,decoded)
autoencoder.compile(optimizer='adam',loss='mse')
autoencoder.fit(x_train,x_train,epochs=5,batch_size=256,shuffle=True,verbose=1)
reconstructed_images = autoencoder.predict(x_test[:10])
n=10
plt.figure(figsize=(20,4))
for i in range(n):
    ax = plt.subplot(2,n,i+1)
    plt.imshow(x_test[i].reshape(28,28),cmap='gray')
    plt.axis('off')
    ax.set_title("Original")
    ax = plt.subplot(2,n,i+1+n)
    plt.imshow(reconstructed_images[i].reshape(28,28),cmap='gray')
    plt.axis('off')
    plt.title('Reconstructed')
plt.show()


------------------------------------------------------------------------------------------------------------------------



5)

from sklearn.feature_extraction.text import CountVectorizer
corpus=['I love Machine Learning.',
        'Machine Learning is fun.',
        'I love programming']
vectorizer=CountVectorizer()
X=vectorizer.fit_transform(corpus)
bow_matrix=X.toarray()


vocabulary=vectorizer.get_feature_names_out()
print("Vocabulary:",vocabulary)
print("Bag of Words Matrix:",bow_matrix)


-----------------------------------------------------------------------------------------------------------------------



6)


import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
np.random.seed(42)
x=np.random.rand(1000,5)
y=3*x[:,0]+2*x[:,1]+1.5*x[:,2]+np.random.randn(1000)*0.1
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
model=keras.Sequential([
    keras.layers.Dense(64,activation='relu',input_shape=(5,)),
    keras.layers.Dense(32,activation='relu'),
    keras.layers.Dense(16,activation='relu'),
    keras.layers.Dense(1)
])
model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])
model.fit(x_train,y_train,epochs=5,batch_size=32,validation_split=0.2)
y_pred=model.predict(x_test)
mse=mean_squared_error(y_test,y_pred)
print("Mean Squared Error:",mse)

-----------------------------------------------------------------------------------------------------------------------
7)  # Import libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# Load and preprocess the data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Reshape to (28,28,1) to match CNN input format and normalize pixel values
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build a simple CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # 10 output classes for digits 0-9
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\nTest accuracy: {test_acc:.4f}")

---------------------------------------------------------------------------------------------------
8) import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load and preprocess MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Reshape to (28,28,1) and normalize
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# Resize to (32,32,3) to match ResNet50 input
x_train = tf.image.resize(x_train, (32, 32))  # Resize to 32x32
x_test = tf.image.resize(x_test, (32, 32))

# Convert grayscale (1 channel) to RGB (3 channels)
x_train = np.repeat(x_train, 3, axis=-1)  # Convert to (32,32,3)
x_test = np.repeat(x_test, 3, axis=-1)

# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Load ResNet50 without top layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
base_model.trainable = False  # Freeze base model

# Create a new model on top of ResNet50
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # 10 classes (digits 0-9)
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), batch_size=64)

# Evaluate on test data
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Test accuracy: {test_acc:.4f}")
------------------------------------------------------------------------------------------------
9) 
!pip install tensorflow pandas numpy scikit-learn
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
# Simple dataset with fake and real news
data = {
    "text": [
        "Breaking news! COVID-19 vaccine turns people into zombies.",
        "NASA confirms life on Mars after recent rover discovery!",
        "Stock market crash predicted by top analysts, sell now!",
        "Scientists develop new AI to cure cancer completely.",
        "Government announces economic reforms to boost growth.",
        "Local hero saves cat stuck in a tree."
    ],
    "label": [1, 1, 1, 1, 0, 0]  # 1 = Fake, 0 = Real
}

df = pd.DataFrame(data)

# Splitting dataset
X_train, X_test, y_train, y_test = train_test_split(df["text"], df["label"], test_size=0.2, random_state=42)
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Padding sequences to the same length
X_train_padded = pad_sequences(X_train_seq, maxlen=10, padding='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=10, padding='post')
model = Sequential([
    Embedding(input_dim=1000, output_dim=16, input_length=10),
    LSTM(8),  # Simple LSTM layer
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
model.fit(X_train_padded, np.array(y_train), epochs=5, batch_size=2)
sample_text = ["COVID-19 vaccine is causing mutations!", "New law passed to improve healthcare."]
sample_seq = tokenizer.texts_to_sequences(sample_text)
sample_padded = pad_sequences(sample_seq, maxlen=10, padding='post')

prediction = model.predict(sample_padded)
for i, text in enumerate(sample_text):
    print(f"News: {text} -> Prediction: {'Fake' if prediction[i][0] > 0.5 else 'Real'}")
------------------------------------------------------------------------------------
10)import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Normalize the images to the range [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0
# Flatten the images (28x28 to 784) and convert labels to one-hot encoding
x_train = x_train.reshape(-1, 28 * 28)
x_test = x_test.reshape(-1, 28 * 28)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
# Build the neural network with dropout layers
model = Sequential([
Dense(512, activation='relu', input_shape=(28 * 28,)), # First hidden layer
Dropout(0.2), # Dropout with 20% probability
Dense(256, activation='relu'), # Second hidden layer
Dropout(0.3), # Dropout with 30% probability
Dense(128, activation='relu'), # Third hidden layer
Dropout(0.4), # Dropout with 40% probability
Dense(10, activation='softmax') # Output layer for 10 classes
])
# Compile the model
model.compile(optimizer='adam',
loss='categorical_crossentropy',
metrics=['accuracy'])
# Train the model
history = model.fit(x_train, y_train,
epochs=10,
batch_size=128,
validation_split=0.2)
# Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")
# Plot training history
plt.figure(figsize=(12, 6)) 
# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()
