According to Record tasks and few additional programs!!
and if sir gives any other dataset you gotta do preprocessing steps accordingly!!!!
1)
*********to the marks dataset apply data cleaning and calculate percentage*********
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
df = pd.read_csv("marks.csv")
df = df.drop_duplicates()
df = df.fillna(df.median(numeric_only=True))
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.dropna(subset=['date'])
df['percentage'] = (df['marks_obtained'] / df['total_marks']) * 100
plt.figure(figsize=(10, 5))
plt.bar(df['student_name'], df['percentage'])
plt.xlabel('Student')
plt.ylabel('Percentage')
plt.title('Percentage vs Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
##Min-Max & Standard Scaling
minmax = MinMaxScaler()
minmax_scaled = minmax.fit_transform(df)
standard = StandardScaler()
standard_scaled = standard.fit_transform(df)
minmax_df = pd.DataFrame(minmax_scaled, columns=df.columns)
standard_df = pd.DataFrame(standard_scaled, columns=df.columns)
print(minmax_df)
print(standard_df)



ORR



import pandas as pd
import numpy as np
df=pd.read_csv('/content/markss.csv')
print("Original dataset")
print(df)
dup=df.drop_duplicates()
print("After removing duplicates")
print(dup)
print("After dropping")
df_drop=df.dropna()
print(df_drop)
df_impu=df.fillna(df.select_dtypes(include=np.number).median())
print("After filling")
print(df_impu)
column_data = df['Roll No']
print(column_data)
___________________________________________________________________________
2)
***********introducing age to marks dataset************
import pandas as pd
from datetime import datetime

df = pd.read_csv("marks.csv")
df['dob'] = pd.to_datetime(df['dob'], format='%Y-%m-%d')
df['age'] = datetime.now().year - df['dob'].dt.year

print(df)
___________________________________________________________________________
3)
**********data cleaning on titanic dataset**************
# Step 1: Import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load the dataset
df = pd.read_csv("titanic-dataset.csv")
print("ðŸ”¹ Original Dataset Preview:")
print(df.head())

# Step 3: Data Cleaning
print("\nðŸ”¹ Missing values before cleaning:")
print(df.isnull().sum())

# Fill missing Age values with mean
df['Age'].fillna(df['Age'].mean(), inplace=True)

# Fill missing Embarked values with mode
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop 'Cabin' column due to excessive nulls
df.drop(columns=['Cabin'], inplace=True)

print("\nðŸ”¹ Missing values after cleaning:")
print(df.isnull().sum())

# Step 4: Data Integration
# (Since no other dataset is mentioned, just include a note)
print("\nðŸ”¹ Data integration: Not applicable (single dataset used)")

# Step 5: Data Transformation
# Convert 'Sex' and 'Embarked' categorical features into numeric
df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})
df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})

print("\nðŸ”¹ Transformed Data Preview:")
print(df[['Sex', 'Embarked']].head())

# Step 6: Data Visualization
print("\nðŸ”¹ Plotting graphs...")

# Survival count
sns.countplot(x='Survived', data=df)
plt.title("Survival Count")
plt.show()

# Age distribution
sns.histplot(df['Age'], bins=20, kde=True)
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

# Class-wise survival
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title("Passenger Class vs Survival")
plt.show()

# Step 7: Save the preprocessed data
df.to_csv("titanic-cleaned.csv", index=False)
print("\n Preprocessed data saved as 'titanic-cleaned.csv'")
___________________________________________________________________________
5)

******One-Sample T-Test*********

from scipy.stats import ttest_1samp

# Sample data
scores = [86, 87, 88, 86, 87, 85, 90, 89]

t_stat, p_val = ttest_1samp(scores, 85)

print("One-Sample T-Test")
print("T-Statistic:", t_stat)
print("P-Value:", p_val)

if p_val < 0.05:
    print("Result: Mean is significantly different from 85.")
else:
    print("Result: No significant difference.")

******Two-Sample T-Test (Unpaired / Independent)******

from scipy.stats import ttest_ind

# Sample data for two groups
group1 = [82, 85, 88, 90, 86]
group2 = [75, 80, 78, 74, 76]

# Unpaired T-test
t_stat, p_val = ttest_ind(group1, group2)

print("\nTwo-Sample T-Test (Unpaired)")
print("T-Statistic:", t_stat)
print("P-Value:", p_val)

if p_val < 0.05:
    print("Result: Significant difference between the two groups.")
else:
    print("Result: No significant difference.")

**********Paired T-Test (Dependent)*********

from scipy.stats import ttest_rel

# Sample before and after scores of same students
before = [72, 75, 78, 79, 80]
after  = [74, 78, 79, 82, 85]

# Paired T-test
t_stat, p_val = ttest_rel(before, after)

print("\nPaired T-Test")
print("T-Statistic:", t_stat)
print("P-Value:", p_val)

if p_val < 0.05:
    print("Result: Significant difference before and after.")
else:
    print("Result: No significant difference.")

_______________________________________________________________
6)

**********Chi-Square Test of Independence********
import pandas as pd
from scipy.stats import chi2_contingency

# Sample contingency table (Gender vs Preference)
data = {'Tea': [30, 10], 'Coffee': [20, 40]}
table = pd.DataFrame(data, index=['Male', 'Female'])

chi2, p, dof, expected = chi2_contingency(table)

print("Chi-Square Test of Independence")
print("Chi2 Statistic:", chi2)
print("P-value:", p)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:\n", expected)

if p < 0.05:
    print("Result: Variables are dependent.")
else:
    print("Result: Variables are independent.")

************Chi-Square Goodness of Fit**********

from scipy.stats import chisquare

# Observed frequencies (e.g., dice rolls)
observed = [18, 22, 20, 19, 21, 20]

# Expected frequencies (assuming fair dice)
expected = [20] * 6

chi2_stat, p_val = chisquare(f_obs=observed, f_exp=expected)

print("\nChi-Square Goodness of Fit Test")
print("Chi2 Statistic:", chi2_stat)
print("P-value:", p_val)

if p_val < 0.05:
    print("Result: Observed data does NOT fit expected distribution.")
else:
    print("Result: Data fits expected distribution.")
___________________________________________________________________________
7)
***********One-Sample Z-Test**************

import numpy as np
from statsmodels.stats.weightstats import ztest

# Sample data
data = [50, 52, 49, 48, 51, 50, 47, 53]
population_mean = 50

# One-sample z-test
z_stat, p_val = ztest(data, value=population_mean)

print("One-Sample Z-Test")
print("Z-statistic:", z_stat)
print("P-value:", p_val)

if p_val < 0.05:
    print("Result: Significant difference from population mean.")
else:
    print("Result: No significant difference.")

********Two-Sample Z-Test*********

# Sample data
group1 = [54, 55, 56, 58, 59]
group2 = [50, 52, 51, 49, 48]

# Two-sample z-test
z_stat, p_val = ztest(group1, group2)

print("\nTwo-Sample Z-Test")
print("Z-statistic:", z_stat)
print("P-value:", p_val)

if p_val < 0.05:
    print("Result: Means are significantly different.")
else:
    print("Result: No significant difference in means.")

___________________________________________________________________
8)
************One-Way ANOVA****************

import pandas as pd
from scipy.stats import f_oneway

# Sample data for three groups
group_A = [23, 25, 27, 24, 22]
group_B = [30, 31, 29, 32, 28]
group_C = [35, 34, 36, 33, 37]

# Perform One-Way ANOVA
f_stat, p_val = f_oneway(group_A, group_B, group_C)

print("One-Way ANOVA")
print("F-Statistic:", f_stat)
print("P-Value:", p_val)

if p_val < 0.05:
    print("Result: Significant difference between group means.")
else:
    print("Result: No significant difference.")

************* Two-Way ANOVA****************
import statsmodels.api as sm
from statsmodels.formula.api import ols
import pandas as pd

# Sample DataFrame
data = {
    'Score': [85, 90, 88, 75, 78, 74, 92, 95, 93, 70, 68, 72],
    'Gender': ['M', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F'],
    'Course': ['Math', 'Math', 'Math', 'Math', 'Math', 'Math',
               'Bio', 'Bio', 'Bio', 'Bio', 'Bio', 'Bio']
}
df = pd.DataFrame(data)

# Perform Two-Way ANOVA
model = ols('Score ~ C(Gender) + C(Course) + C(Gender):C(Course)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

print("\nTwo-Way ANOVA")
print(anova_table)
_______________________________________________________________________________________
9)
***********mean vector,covariance,correlation******************
import numpy as np
import pandas as pd

# Sample dataset
data = {
    'X1': [2, 4, 6, 8, 10],
    'X2': [1, 3, 5, 7, 9],
    'X3': [10, 20, 30, 40, 50]
}

# Create DataFrame
df = pd.DataFrame(data)
print("Original Dataset:\n", df)

# Mean Vector
mean_vector = df.mean()
print("\nMean Vector:\n", mean_vector)

# Variance of each feature
variance = df.var(ddof=1)  # Sample variance
print("\nVariance:\n", variance)

# Covariance Matrix
covariance_matrix = df.cov()
print("\nCovariance Matrix:\n", covariance_matrix)

# Correlation Matrix
correlation_matrix = df.corr()
print("\nCorrelation Matrix:\n", correlation_matrix)
_________________________________________________________________________________
10)
**************************PCA******************
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Step 1: Sample dataset
data = {
    'X1': [2, 4, 6, 8, 10],
    'X2': [1, 3, 5, 7, 9],
    'X3': [10, 20, 30, 40, 50]
}
df = pd.DataFrame(data)
print("Original Data:\n", df)

# Step 2: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Step 3: Apply PCA
pca = PCA()
pca_components = pca.fit_transform(X_scaled)

# Step 4: Display results
print("\nPCA Components (transformed data):\n", pd.DataFrame(pca_components, columns=['PC1', 'PC2', 'PC3']))
print("\nExplained Variance Ratio:\n", pca.explained_variance_ratio_)
__________________________________________________________________________________________________________
11)
**************apriori algorithm****************

!pip install mlxtend
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# Step 1: Sample transaction data
transactions = [
    ['milk', 'bread', 'butter'],
    ['bread', 'butter'],
    ['milk', 'bread'],
    ['milk', 'bread', 'butter'],
    ['bread']
]

# Step 2: Convert to one-hot encoded DataFrame
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df = pd.DataFrame(te_ary, columns=te.columns_)

print("Transaction Data:\n", df)

# Step 3: Apply Apriori algorithm
frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)
print("\nFrequent Itemsets:\n", frequent_itemsets)

# Step 4: Generate Association Rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
print("\nAssociation Rules:\n", rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
________________________________________________________________________________
12)

*****************SVM********************

import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load sample dataset (Iris)
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Step 2: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 3: Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 4: Apply SVM
svm_model = SVC(kernel='linear')  # You can also use 'rbf', 'poly', etc.
svm_model.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = svm_model.predict(X_test)

# Step 6: Evaluate model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________________________________________________
13)
****************KNN classifier********************

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load dataset (Iris)
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Step 2: Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Step 3: Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 4: Create and train KNN model
knn = KNeighborsClassifier(n_neighbors=3)  # You can change K value
knn.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = knn.predict(X_test)

# Step 6: Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
________________________________________________________________________________________________
14)
**************Gaussian Naive Bayes (using Iris dataset)*******************

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

# Load dataset
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)

# Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

print("=== Gaussian Naive Bayes ===")
print(classification_report(y_test, y_pred))

***************Multinomial Naive Bayes (using text data)*******************

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

# Sample text data
texts = ['I love programming', 'Python is great', 'I hate bugs', 'Debugging is hard', 'I love Python']
labels = [1, 1, 0, 0, 1]  # 1: Positive, 0: Negative

# Multinomial Naive Bayes Pipeline
model = make_pipeline(CountVectorizer(), MultinomialNB())
model.fit(texts, labels)

# Predict
preds = model.predict(['I love debugging', 'Bugs are annoying'])
print("\n=== Multinomial Naive Bayes ===")
print("Predictions:", preds)

****************Bernoulli Naive Bayes (binary features)***********************
from sklearn.naive_bayes import BernoulliNB
import numpy as np

# Binary feature data (e.g., word presence)
X = np.array([
    [1, 0, 1, 0],
    [1, 1, 0, 1],
    [0, 1, 1, 0],
    [0, 0, 0, 1]
])
y = [1, 1, 0, 0]  # Labels

# Bernoulli Naive Bayes
bnb = BernoulliNB()
bnb.fit(X, y)
y_pred = bnb.predict([[1, 0, 0, 1]])
print("\n=== Bernoulli Naive Bayes ===")
print("Prediction:", y_pred)
______________________________________________________________________________________________

15)
***************Decision and random forest****************************

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ======================
#  Decision Tree
# ======================
dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)  # You can also use 'entropy'
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

print("=== Decision Tree Classifier ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("Classification Report:\n", classification_report(y_test, y_pred_dt))

# ======================
#  Random Forest
# ======================
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("=== Random Forest Classifier ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

________________________________________________________________________________________________________
16)
*****************Binary Logistic Regression (2 classes)***************************

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

# Load binary classification dataset
data = load_breast_cancer()
X = data.data
y = data.target  # Binary: 0 = malignant, 1 = benign

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Binary Logistic Regression
model_bin = LogisticRegression()
model_bin.fit(X_train, y_train)
y_pred_bin = model_bin.predict(X_test)

print("=== Binary Logistic Regression ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_bin))
print("Classification Report:\n", classification_report(y_test, y_pred_bin))

******************Multinomial Logistic Regression (more than 2 classes)**************************

from sklearn.datasets import load_iris

# Load Iris dataset (3 classes)
iris = load_iris()
X = iris.data
y = iris.target  # Multiclass: 0, 1, 2

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scaling
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Multinomial Logistic Regression
model_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)
model_multi.fit(X_train, y_train)
y_pred_multi = model_multi.predict(X_test)

print("\n=== Multinomial Logistic Regression ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_multi))
print("Classification Report:\n", classification_report(y_test, y_pred_multi))
____________________________________________________________________________________________________________________
17)
***************(All Smoothing Techniques)*********************
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing

# Sample time series data
data = [112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118,
        115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, 140]

df = pd.DataFrame(data, columns=['Passengers'])
df['Month'] = pd.date_range(start='2020-01', periods=len(df), freq='M')
df.set_index('Month', inplace=True)

# --- 1. Simple Moving Average (window=3)
df['SMA_3'] = df['Passengers'].rolling(window=3).mean()

# --- 2. Weighted Moving Average (manual)
weights = np.array([0.1, 0.3, 0.6])  # More weight to recent
def weighted_moving_average(series, weights):
    return np.convolve(series, weights[::-1], mode='valid')

df['WMA_3'] = pd.Series(weighted_moving_average(df['Passengers'], weights), index=df.index[2:])

# --- 3. Exponential Smoothing (Single)
model_ses = SimpleExpSmoothing(df['Passengers']).fit(smoothing_level=0.2, optimized=False)
df['SES'] = model_ses.fittedvalues

# --- 4. Double Exponential Smoothing (Holtâ€™s Linear Trend)
model_holt = Holt(df['Passengers']).fit(smoothing_level=0.8, smoothing_trend=0.2)
df['Holt'] = model_holt.fittedvalues

# --- 5. Triple Exponential Smoothing (Holt-Winters Seasonal)
model_hw = ExponentialSmoothing(df['Passengers'], seasonal='add', seasonal_periods=12).fit()
df['Holt_Winters'] = model_hw.fittedvalues

# --- Plot all
plt.figure(figsize=(12, 6))
plt.plot(df['Passengers'], label='Original', linewidth=2)
plt.plot(df['SMA_3'], label='Simple Moving Avg')
plt.plot(df['WMA_3'], label='Weighted Moving Avg')
plt.plot(df['SES'], label='Exponential Smoothing')
plt.plot(df['Holt'], label='Double Exp Smoothing')
plt.plot(df['Holt_Winters'], label='Holt-Winters')
plt.legend()
plt.title("Smoothing Techniques in Time Series")
plt.grid()
plt.show()
____________________________________________________________________________________________________________

19)
********************Auto Regressive & Moving Average Models********************
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.ar_model import AutoReg
from statsmodels.tsa.arima.model import ARIMA

# Sample time series data (monthly sales or similar)
data = [266, 145, 183, 119, 180, 168, 232, 197, 151, 199, 134, 158,
        210, 180, 234, 267, 160, 170, 219, 263, 192, 118, 190, 221]

df = pd.DataFrame(data, columns=['Value'])

# Plot the data
df.plot(title="Time Series Data", figsize=(8, 4))
plt.show()

# AR model
ar_model = AutoReg(df['Value'], lags=1).fit()
ar_forecast = ar_model.predict(start=len(df), end=len(df)+5)

print("AutoRegressive Forecast:")
print(ar_forecast)

# MA model using ARIMA (AR=0, I=0, MA=1)
ma_model = ARIMA(df['Value'], order=(0, 0, 1)).fit()
ma_forecast = ma_model.forecast(steps=6)

print("\nMoving Average Forecast:")
print(ma_forecast)
_____________________________________________________________________________________________________________
18)
***************** ARIMA Time Series Forecasting**********************
from statsmodels.tsa.arima.model import ARIMA

# Using same df['Value'] as before

# Fit ARIMA model (order p,d,q) = (2,1,2) as an example
arima_model = ARIMA(df['Value'], order=(2, 1, 2))
arima_result = arima_model.fit()

# Forecast next 6 time points
arima_forecast = arima_result.forecast(steps=6)

# Plot original + forecast
plt.figure(figsize=(8, 4))
plt.plot(df['Value'], label='Original')
plt.plot(range(len(df), len(df)+6), arima_forecast, label='Forecast', color='red')
plt.title("ARIMA Forecast")
plt.legend()
plt.grid()
plt.show()

print("\nARIMA Forecasted Values:")
print(arima_forecast)
___________________________________________________________________________________________________________________________
19)
************************** Data Visualization using a Tool (Matplotlib/Seaborn)********************************

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample dataset
data = {
    'Department': ['CSE', 'ECE', 'EEE', 'MECH', 'CIVIL'],
    'Students': [120, 100, 80, 60, 40]
}
df_viz = pd.DataFrame(data)

# --- Bar Chart (Seaborn with Matplotlib) ---
plt.figure(figsize=(6, 4))
sns.barplot(x='Department', y='Students', data=df_viz, palette='Set2')
plt.title("Department-wise Student Count")
plt.xlabel("Department")
plt.ylabel("Number of Students")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# --- Pie Chart (Matplotlib) ---
plt.figure(figsize=(5, 5))
plt.pie(df_viz['Students'], labels=df_viz['Department'], autopct='%1.1f%%', startangle=90)
plt.title("Department Distribution")
plt.axis('equal')
plt.tight_layout()
plt.show()

# --- Line Chart (Optional: Matplotlib) ---
plt.figure(figsize=(6, 4))
plt.plot(df_viz['Department'], df_viz['Students'], marker='o', linestyle='-', color='purple')
plt.title("Student Count by Department (Line Chart)")
plt.xlabel("Department")
plt.ylabel("Number of Students")
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
